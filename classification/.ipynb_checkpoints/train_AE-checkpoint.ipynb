{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torch_data\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([True, False, True])\n",
    "b = torch.tensor([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(Data, self).__init__()\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.ones((6,1,192,192,192))\n",
    "y_train = np.array([1,2,10,2,3,4])\n",
    "X_val  = np.ones((1,1,192,192,192))\n",
    "y_val = np.array([12])\n",
    "train_dataset = Data(X_train, y_train)\n",
    "val_dataset = Data(X_val, y_val)\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  \n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/models')\n",
    "from AE_model import AE, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss,train_loss_disc, val_loss, val_loss_disc):\n",
    "    display.clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    if train_loss:\n",
    "        ax[0,0].semilogy(train_loss)\n",
    "        ax[0,0].set_title('Training loss')\n",
    "        ax[0,0].set_xlabel('# batches processed')\n",
    "        ax[0,0].set_ylabel('loss value')\n",
    "    if train_loss_disc:\n",
    "        ax[0,1].semilogy(train_loss_disc)\n",
    "        ax[0,1].set_title('Training discriminator loss')\n",
    "        ax[0,1].set_xlabel('# batches processed')\n",
    "        ax[0,1].set_ylabel('loss value')\n",
    "    if val_loss:\n",
    "        ax[1,0].semilogy(val_loss)\n",
    "        ax[1,0].set_title('Validation  loss')\n",
    "        ax[1,0].set_xlabel('# batches processed')\n",
    "        ax[1,0].set_ylabel('loss value')\n",
    "    if val_loss_disc:\n",
    "        ax[1,1].semilogy(val_loss_disc)\n",
    "        ax[1,1].set_title('Validation discriminator loss')\n",
    "        ax[1,1].set_xlabel('# batches processed')\n",
    "        ax[1,1].set_ylabel('loss value')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_loss(y, pred_logits):\n",
    "    global n_domains\n",
    "    global device\n",
    "    y_onehot = torch.zeros((y.shape[0], n_domains), dtype=torch.int32)\n",
    "    y_onehot.scatter_(1, y.view(-1,1), 1)\n",
    "    y_reverse = (1-y_onehot).to(device)\n",
    "    pred_prob = F.log_softmax(pred_logits,dim=1)\n",
    "    return -torch.mean(torch.mul(y_reverse, pred_prob))\n",
    "    del y_onehot, y_reverse, pred_prob\n",
    "\n",
    "def main_loss(X_rec, X, rec_criterion_ae, \n",
    "              y, pred_logits, lambda_t):\n",
    "    \n",
    "    loss_rec = criterion_ae(X_rec, X) #MSE loss of reconstraction\n",
    "    loss_adversarial = adv_loss(y, pred_logits)\n",
    "    return loss_rec + lambda_t*loss_adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ae, disc, criterion_ae, criterion_disc, optimizer_ae, optimizer_disc,\n",
    "        train_loader, val_loader,epochs, disc_loop=1,scheduler=None, exp_name=None,   save_dir=None):\n",
    "    global device\n",
    "    lambda_final = 1e-4\n",
    "    lambda_initial = 0\n",
    "    max_step = 500000\n",
    "    lambda_step = (lambda_final-lambda_initial)/max_step\n",
    "    step = 0\n",
    "    lambda_t = lambda_initial\n",
    "    \n",
    "#     writer = SummaryWriter(f'logs/{exp_name}')\n",
    "    train_loss = []\n",
    "    train_loss_disc = []\n",
    "    val_loss = []\n",
    "    val_loss_disc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_no, (X, y) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            '''\n",
    "            X - ground truth image\n",
    "            y - label of device, single scalar  \n",
    "            '''\n",
    "            X = X.to(device)\n",
    "            \n",
    "            ###train discrimintor\n",
    "            ae.eval()\n",
    "            for param in ae.parameters(): #fix encoder parameters to train disc\n",
    "                param.requires_grad = False\n",
    "            disc.train()\n",
    "            for j in range(disc_loop): ### how in GAN do several iterations for discriminator?\n",
    "                optimizer_disc.zero_grad()\n",
    "                pred_logits = disc(ae.enc(X)[0])\n",
    "                loss_disc = criterion_disc(pred_logits, y.to(device))#Cross entropy loss, disc learn to pred real domain\n",
    "                loss_disc.backward()\n",
    "                optimizer_disc.step()\n",
    "            for param in ae.parameters():\n",
    "                param.requires_grad = True\n",
    "            del pred_logits\n",
    "            ###train AE\n",
    "            ae.train()\n",
    "            disc.eval()\n",
    "            for param in disc.parameters(): # fix discriminator parameters to train autoencoder\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            optimizer_ae.zero_grad()\n",
    "            X_rec = ae(X)\n",
    "            pred_logits = disc(ae.enc(X)[0])\n",
    "            loss = main_loss(X_rec, X, criterion_ae, y, pred_logits, lambda_t)\n",
    "            loss.backward()\n",
    "            optimizer_ae.step()\n",
    "            for param in disc.parameters():\n",
    "                param.requires_grad = True\n",
    "            del X_rec, pred_logits\n",
    "            ### increase lambda\n",
    "            step += 1\n",
    "            if step < max_step:\n",
    "                lambda_t += lambda_step\n",
    "                \n",
    "            ###Plot\n",
    "            if batch_no % 10 == 0:\n",
    "                train_loss.append(loss.item())\n",
    "                train_loss_disc.append(loss_disc.item())\n",
    "                plot_loss(train_loss, train_loss_disc,val_loss, val_loss_disc,)\n",
    "                print(f'epoch {epoch} training stage...')\n",
    "#                 writer.add_scalar('train loss', loss.item(), global_step=len(train_loss))\n",
    "#                 writer.add_scalar('train clas loss', loss_disc.item(), global_step=len(train_loss))\n",
    "\n",
    "        \n",
    "        print(f'epoch {epoch} testing stage...')\n",
    "        ae.eval()\n",
    "        disc.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_no, (X, y) in tqdm(enumerate(val_loader), \n",
    "                                                     total=len(val_loader)):\n",
    "               \n",
    "                X = X.to(device)\n",
    "                pred_logits = disc(ae.enc(X)[0])\n",
    "                loss_disc = criterion_disc(pred_logits, y.to(device))\n",
    "                X_rec = ae(X)\n",
    "                loss = main_loss(X_rec, X, criterion_ae, y, pred_logits, lambda_t)\n",
    "                val_loss.append(loss.item())\n",
    "                val_loss_disc.append(loss_disc.item())\n",
    "            del X_rec, pred_logits\n",
    "#                 writer.add_scalar('val loss', loss.item(), global_step=len(val_loss))\n",
    "#                 writer.add_scalar('val clas loss', loss_disc.item(), global_step=len(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_block_kwargs = {\n",
    "    'conv_k': 3,\n",
    "    'conv_pad': 1,\n",
    "    'conv_s': 1,\n",
    "    'maxpool_k': 2,\n",
    "    'maxpool_s': 2,\n",
    "    'batch_norm': True,\n",
    "    'act': 'relu' ##'or l_relu'\n",
    "}\n",
    "up_block_kwargs = {\n",
    "    'up': 'upsample',# or 'transpose_conv'\n",
    "    'scale': 2,\n",
    "    'scale_mode': 'nearest',\n",
    "    'conv_k': 3,\n",
    "    'conv_pad': 1,\n",
    "    'conv_s': 1,\n",
    "    'batch_norm': True,\n",
    "    'act': 'relu' ##'or l_relu'\n",
    "}\n",
    "ae_kwargs ={\n",
    "    'c_in':1,\n",
    "    'is_skip': False,\n",
    "    'deapth': 6,\n",
    "    'c_base': 16,\n",
    "    'inc_size':2,\n",
    "    'reduce_size': False,\n",
    "    'down_block_kwargs': down_block_kwargs,\n",
    "    'up_block_kwargs': up_block_kwargs,\n",
    "}\n",
    "discriminator_kwargs = {\n",
    "    'c_in': 512,\n",
    "    'c_out':1024,\n",
    "    'conv_k': 2,\n",
    "    'conv_s': 2,\n",
    "    'conv_pad': 0,\n",
    "    'l_in': 1024,\n",
    "    'l_out': 512,\n",
    "    'batch_norm': False,\n",
    "    'act': 'l_relu',\n",
    "    'n_domains':20\n",
    "    \n",
    "}\n",
    "n_domains = 20\n",
    "ae = AE(**ae_kwargs)\n",
    "disc = Discriminator(**discriminator_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 36691771392 bytes. Buy new RAM!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-b610cff05f83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0moptimizer_disc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate_disc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbetas_disc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m train(ae, disc, criterion_ae, criterion_disc, optimizer_ae, optimizer_disc,\n\u001b[1;32m---> 16\u001b[1;33m         train_loader, val_loader,epochs=2, disc_loop=1)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-76-9df1f420495d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(ae, disc, criterion_ae, criterion_disc, optimizer_ae, optimizer_disc, train_loader, val_loader, epochs, disc_loop, scheduler, exp_name, save_dir)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0moptimizer_ae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mX_rec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0mpred_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_rec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_ae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\mri-epilepsy-segmentation\\classification\\models\\AE_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\mri-epilepsy-segmentation\\classification\\models\\AE_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, size_list)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0msize_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\mri-epilepsy-segmentation\\classification\\models\\AE_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, shape_before_pool, x_before_pool)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m#                 assert x_before_pool == None, 'wrang skip_map'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;31m#                 x = torch.cat([x, x_before_pool], dim=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             if key =='up' and (shape_before_pool[0] > x.shape[1] or \n\u001b[0;32m     73\u001b[0m                                \u001b[0mshape_before_pool\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    478\u001b[0m                             self.dilation, self.groups)\n\u001b[0;32m    479\u001b[0m         return F.conv3d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 480\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 36691771392 bytes. Buy new RAM!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "ae.to(device)\n",
    "disc.to(device)\n",
    "\n",
    "learning_rate_ae = 0.001\n",
    "betas_ae = (0.9,0.999)\n",
    "learning_rate_disc = 0.001\n",
    "betas_disc = (0.9,0.999)\n",
    "\n",
    "criterion_ae = nn.MSELoss()\n",
    "criterion_disc = nn.CrossEntropyLoss()\n",
    "optimizer_ae = torch.optim.Adam(ae.parameters(), lr=learning_rate_ae, betas=betas_ae)\n",
    "optimizer_disc = torch.optim.Adam(disc.parameters(), lr=learning_rate_disc, betas=betas_disc)\n",
    "train(ae, disc, criterion_ae, criterion_disc, optimizer_ae, optimizer_disc,\n",
    "        train_loader, val_loader,epochs=2, disc_loop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 32\n",
    "# n_domains = 20\n",
    "# enc = Encoder(c = c)\n",
    "# dec = Decoder(c = c)\n",
    "# disc = Discriminator(n_domains = n_domains)\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc.to(device)\n",
    "# summary(enc, (1,256,256,256),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec.to(device)\n",
    "# summary(dec, (256,4,4,4),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disc.to(device)\n",
    "# summary(disc, (256,4, 4, 4),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
